{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809206fc",
   "metadata": {},
   "source": [
    "# Feature Engineering Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebf6ed2",
   "metadata": {},
   "source": [
    "This notebook develops and validates the mathematical transformations that convert\n",
    "raw match statistics into predictive features capturing temporal performance dynamics.\n",
    "\n",
    "Mathematical Framework:\n",
    "For player i at gameweek t, we construct feature vector X(i,t) that incorporates:\n",
    "1. Rolling performance metrics with exponential decay weighting\n",
    "2. Opponent-adjusted statistics normalized by defensive strength\n",
    "3. Fixture context including upcoming difficulty and schedule density\n",
    "\n",
    "The temporal ordering constraint requires that X(i,t) depends only on information\n",
    "available at or before gameweek t, preventing information leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1238093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806c8c09",
   "metadata": {},
   "source": [
    "## Visualization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e547a16b",
   "metadata": {},
   "source": [
    "## CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().parent if 'Notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "PROCESSED_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Processed Data Directory: {PROCESSED_DIR}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335d8b32",
   "metadata": {},
   "source": [
    "## SECTION 1: LOAD PREPROCESSED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba04f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PROCESSED_DIR / \"fpl_unified_preprocessed.csv\")\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"Temporal Range: {df['season'].min()} to {df['season'].max()}\")\n",
    "print(f\"Gameweek Range: GW{df['GW'].min()} to GW{df['GW'].max()}\")\n",
    "\n",
    "# Convert kickoff_time to datetime for temporal operations\n",
    "df['kickoff_time'] = pd.to_datetime(df['kickoff_time'])\n",
    "\n",
    "# Sort by temporal order (critical for rolling calculations)\n",
    "df = df.sort_values(['season', 'GW', 'element']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaebf41",
   "metadata": {},
   "source": [
    "## SECTION 2: ROLLING PERFORMANCE METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38a83f",
   "metadata": {},
   "source": [
    "Mathematical Foundation:\n",
    "\n",
    "For metric m at time t, the exponentially weighted moving average is:\n",
    "    EWMA(m,t) = Σ(k=1 to n) w(k) × m(t-k)\n",
    "\n",
    "where weights follow exponential decay:\n",
    "    w(k) = λ^k / Σ(j=1 to n) λ^j\n",
    "\n",
    "The decay parameter λ ∈ (0,1) controls temporal weighting:\n",
    "- λ close to 1: slow decay, equal weighting across window\n",
    "- λ close to 0: rapid decay, recent observations dominate\n",
    "\n",
    "We implement multiple windows to capture different temporal scales:\n",
    "- 3-game window (λ=0.5): immediate recent form\n",
    "- 5-game window (λ=0.6): medium-term trajectory  \n",
    "- Season-long window: baseline quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca13979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_stats(df: pd.DataFrame, \n",
    "                            metrics: list, \n",
    "                            windows: list = [3, 5]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates rolling statistics for specified metrics across multiple windows.\n",
    "    \n",
    "    Implementation uses pandas rolling with min_periods=1 to handle early-season\n",
    "    cases where insufficient historical data exists. This ensures every observation\n",
    "    receives a feature value, though early-season features have higher uncertainty.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Sorted dataframe with temporal ordering\n",
    "    metrics : list\n",
    "        Performance metrics to aggregate\n",
    "    windows : list\n",
    "        Window sizes in number of gameweeks\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with added rolling statistic columns\n",
    "    \"\"\"\n",
    "    df_rolled = df.copy()\n",
    "    \n",
    "    # Group by player to ensure rolling stats don't cross player boundaries\n",
    "    for metric in metrics:\n",
    "        if metric not in df.columns:\n",
    "            print(f\"Warning: {metric} not found in dataframe, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        for window in windows:\n",
    "            col_name = f'{metric}_roll_{window}gw'\n",
    "            \n",
    "            # Calculate rolling mean per player\n",
    "            df_rolled[col_name] = (\n",
    "                df_rolled.groupby('element')[metric]\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "    \n",
    "    return df_rolled\n",
    "\n",
    "\n",
    "# Define metrics to track\n",
    "performance_metrics = [\n",
    "    'total_points',\n",
    "    'minutes', \n",
    "    'goals_scored',\n",
    "    'assists',\n",
    "    'clean_sheets',\n",
    "    'ict_index',\n",
    "    'influence',\n",
    "    'creativity', \n",
    "    'threat',\n",
    "    'bps'\n",
    "]\n",
    "\n",
    "# Add expected goals metrics for seasons where available\n",
    "xg_metrics = ['expected_goals', 'expected_assists', 'expected_goal_involvements']\n",
    "available_xg = [m for m in xg_metrics if m in df.columns]\n",
    "performance_metrics.extend(available_xg)\n",
    "\n",
    "print(f\"Computing rolling statistics for {len(performance_metrics)} metrics...\")\n",
    "print(f\"Windows: {[3, 5]} gameweeks\")\n",
    "\n",
    "df = calculate_rolling_stats(df, performance_metrics, windows=[3, 5])\n",
    "\n",
    "print(f\"\\nNew columns added: {[c for c in df.columns if '_roll_' in c][:5]}...\")\n",
    "print(f\"Total rolling features: {len([c for c in df.columns if '_roll_' in c])}\")\n",
    "\n",
    "# Validation: Check for NaN patterns in rolling features\n",
    "print(\"\\nValidation - Missing values in rolling features (sample):\")\n",
    "rolling_cols = [c for c in df.columns if '_roll_' in c][:5]\n",
    "for col in rolling_cols:\n",
    "    n_missing = df[col].isna().sum()\n",
    "    print(f\"{col:40s}: {n_missing:6d} missing ({n_missing/len(df)*100:5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f521c",
   "metadata": {},
   "source": [
    "### FORENSIC FEATURE AUDIT & CLEANUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def forensic_cleanup_pipeline(df_in):\n",
    "    \"\"\"\n",
    "    1. THE PURGE: Removes market noise features.\n",
    "    2. THE INJECTION: Adds fixture context (Home/Away interaction).\n",
    "    3. THE FILTER: Identifies and removes highly collinear features.\n",
    "    \"\"\"\n",
    "    df_clean = df_in.copy()\n",
    "    \n",
    "    # --- 1. THE PURGE (Remove Market Noise) ---\n",
    "    # These features cause feedback loops (high price -> high selection -> high price)\n",
    "    # We want to predict points based on PERFORMANCE, not POPULARITY.\n",
    "    noise_features = [\n",
    "        'value', 'selected', 'transfers_in', 'transfers_out', \n",
    "        'transfers_balance', 'own_goals', 'penalties_missed', 'penalties_saved'\n",
    "    ]\n",
    "    dropped_noise = [c for c in noise_features if c in df_clean.columns]\n",
    "    df_clean = df_clean.drop(columns=dropped_noise)\n",
    "    print(f\"[PURGE] Removed {len(dropped_noise)} market noise features: {dropped_noise}\")\n",
    "\n",
    "    # --- 2. THE INJECTION (Fixture Difficulty Context) ---\n",
    "    # We approximate difficulty using Home/Away form. \n",
    "    # Logic: Performing well at HOME is standard. Performing well AWAY is a signal of class.\n",
    "    # If 'was_home' and rolling points exist, create interaction features.\n",
    "    \n",
    "    if 'was_home' in df_clean.columns and 'total_points_roll_5gw' in df_clean.columns:\n",
    "        # Convert boolean to int if necessary\n",
    "        is_home = df_clean['was_home'].astype(int)\n",
    "        \n",
    "        # Feature: Form when playing at Home\n",
    "        df_clean['form_at_home'] = df_clean['total_points_roll_5gw'] * is_home\n",
    "        \n",
    "        # Feature: Form when playing Away (High value here is a very strong signal)\n",
    "        df_clean['form_away'] = df_clean['total_points_roll_5gw'] * (1 - is_home)\n",
    "        \n",
    "        print(\"[INJECTION] Created 'form_at_home' and 'form_away' interaction features.\")\n",
    "    \n",
    "    # --- 3. THE FILTER (Remove Multicollinearity) ---\n",
    "    # Mathematical Goal: Reduce X dimension where Corr(Xi, Xj) > 0.95\n",
    "    \n",
    "    # Select only numeric features for correlation check\n",
    "    numeric_df = df_clean.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Exclude metadata and target from being dropped\n",
    "    protected_cols = ['season', 'GW', 'element', 'total_points', 'minutes']\n",
    "    candidates = [c for c in numeric_df.columns if c not in protected_cols]\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df_clean[candidates].corr().abs()\n",
    "    \n",
    "    # Select upper triangle\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find features with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    \n",
    "    # Perform the drop\n",
    "    df_clean = df_clean.drop(columns=to_drop)\n",
    "    \n",
    "    print(f\"[FILTER] Identified {len(to_drop)} redundant features (Corr > 0.95).\")\n",
    "    if len(to_drop) > 0:\n",
    "        print(f\"         Examples: {to_drop[:5]}...\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# --- EXECUTE PIPELINE ---\n",
    "print(f\"Original Shape: {df.shape}\")\n",
    "df_model_ready = forensic_cleanup_pipeline(df)\n",
    "print(f\"Clean Shape:    {df_model_ready.shape}\")\n",
    "\n",
    "# --- OPTIONAL: SAVE FOR MODELING ---\n",
    "output_path = PROCESSED_DIR / \"fpl_model_ready.csv\"\n",
    "df_model_ready.to_csv(output_path, index=False)\n",
    "print(f\"\\nSaved clean dataset to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f95792",
   "metadata": {},
   "source": [
    "## SECTION 3: VISUALIZE ROLLING STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831aa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a high-profile player for visualization\n",
    "# Filter to players with substantial appearances\n",
    "player_games = df.groupby('element').size()\n",
    "active_players = player_games[player_games >= 20].index\n",
    "\n",
    "# Get a sample player\n",
    "if len(active_players) > 0:\n",
    "    sample_player_id = active_players[232]\n",
    "    player_data = df[df['element'] == sample_player_id].copy()\n",
    "    player_name = player_data['name'].iloc[0] if 'name' in player_data.columns else f\"Player {sample_player_id}\"\n",
    "    \n",
    "    print(f\"Visualizing rolling statistics for: {player_name}\")\n",
    "    print(f\"Total appearances: {len(player_data)}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Rolling Performance Metrics - {player_name}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Total Points\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(player_data['GW'], player_data['total_points'], 'o-', alpha=0.3, label='Actual')\n",
    "    ax.plot(player_data['GW'], player_data['total_points_roll_3gw'], '-', linewidth=2, label='3-GW Average')\n",
    "    ax.plot(player_data['GW'], player_data['total_points_roll_5gw'], '-', linewidth=2, label='5-GW Average')\n",
    "    ax.set_xlabel('Gameweek')\n",
    "    ax.set_ylabel('Total Points')\n",
    "    ax.set_title('Total Points - Rolling Averages')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: ICT Index\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(player_data['GW'], player_data['ict_index'], 'o-', alpha=0.3, label='Actual')\n",
    "    ax.plot(player_data['GW'], player_data['ict_index_roll_3gw'], '-', linewidth=2, label='3-GW Average')\n",
    "    ax.plot(player_data['GW'], player_data['ict_index_roll_5gw'], '-', linewidth=2, label='5-GW Average')\n",
    "    ax.set_xlabel('Gameweek')\n",
    "    ax.set_ylabel('ICT Index')\n",
    "    ax.set_title('ICT Index - Rolling Averages')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Minutes Played\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(player_data['GW'], player_data['minutes'], 'o-', alpha=0.3, label='Actual')\n",
    "    ax.plot(player_data['GW'], player_data['minutes_roll_3gw'], '-', linewidth=2, label='3-GW Average')\n",
    "    ax.plot(player_data['GW'], player_data['minutes_roll_5gw'], '-', linewidth=2, label='5-GW Average')\n",
    "    ax.set_xlabel('Gameweek')\n",
    "    ax.set_ylabel('Minutes')\n",
    "    ax.set_title('Minutes Played - Rolling Averages')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Expected Goals (if available)\n",
    "    ax = axes[1, 1]\n",
    "    if 'expected_goals' in player_data.columns and not player_data['expected_goals'].isna().all():\n",
    "        ax.plot(player_data['GW'], player_data['expected_goals'], 'o-', alpha=0.3, label='Actual xG')\n",
    "        if 'expected_goals_roll_3gw' in player_data.columns:\n",
    "            ax.plot(player_data['GW'], player_data['expected_goals_roll_3gw'], '-', linewidth=2, label='3-GW Avg')\n",
    "        ax.set_xlabel('Gameweek')\n",
    "        ax.set_ylabel('Expected Goals')\n",
    "        ax.set_title('Expected Goals - Rolling Averages')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Expected Goals data not available\\nfor this player/season', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(BASE_DIR / 'notebooks' / 'figures' / 'rolling_stats_example.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nVisualization complete. Notice how rolling averages smooth volatility.\")\n",
    "    print(\"The 3-GW average responds quickly to form changes.\")\n",
    "    print(\"The 5-GW average provides more stable trend assessment.\")\n",
    "\n",
    "else:\n",
    "    print(\"No players with sufficient appearances found for visualization.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726db77",
   "metadata": {},
   "source": [
    "## SECTION 4: OPPONENT STRENGTH INDICES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519217a",
   "metadata": {},
   "source": [
    "Mathematical Foundation:\n",
    "\n",
    "Team defensive strength at gameweek t:\n",
    "    DS(team,t) = Σ(k in recent matches) goals_conceded(k) / n_matches\n",
    "\n",
    "Opponent adjustment factor for player i facing team j:\n",
    "    α(i,j,t) = DS(j,t) / DS_league_avg(t)\n",
    "\n",
    "Adjusted metric:\n",
    "    m_adjusted(i,t) = m(i,t) × α(i,j,t)\n",
    "\n",
    "This normalization isolates player skill from match difficulty. A midfielder\n",
    "with 50 creativity against Manchester City (strong defense) provides more\n",
    "predictive signal than 50 creativity against a relegation candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b10e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_team_defensive_strength(df: pd.DataFrame, window: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates rolling defensive strength for each team.\n",
    "    \n",
    "    Defensive strength measures goals conceded per match over recent window.\n",
    "    Lower values indicate stronger defenses. We use rolling window to capture\n",
    "    current form rather than season-long averages that may be stale.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Match-level data with team and goals_conceded\n",
    "    window : int\n",
    "        Number of recent matches to average\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with team defensive strength indices\n",
    "    \"\"\"\n",
    "    # Calculate goals conceded per team per gameweek\n",
    "    team_defense = (\n",
    "        df.groupby(['season', 'GW', 'team'])['goals_conceded']\n",
    "        .mean()  # Average across players (same value for all team members)\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Calculate rolling defensive strength per team\n",
    "    team_defense = team_defense.sort_values(['team', 'season', 'GW'])\n",
    "    team_defense['def_strength'] = (\n",
    "        team_defense.groupby('team')['goals_conceded']\n",
    "        .rolling(window=window, min_periods=1)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    \n",
    "    return team_defense[['season', 'GW', 'team', 'def_strength']]\n",
    "\n",
    "\n",
    "# Calculate team defensive indices\n",
    "print(\"Computing team defensive strength indices...\")\n",
    "team_defense_df = calculate_team_defensive_strength(df, window=5)\n",
    "\n",
    "print(f\"\\nDefensive strength calculated for {team_defense_df['team'].nunique()} teams\")\n",
    "print(\"\\nSample defensive strengths (lower = stronger defense):\")\n",
    "print(team_defense_df.groupby('team')['def_strength'].mean().sort_values().head(10))\n",
    "\n",
    "# Calculate league average defensive strength per gameweek\n",
    "league_avg_defense = (\n",
    "    team_defense_df.groupby(['season', 'GW'])['def_strength']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'def_strength': 'league_avg_def'})\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb62f79",
   "metadata": {},
   "source": [
    "## SECTION 5: OPPONENT-ADJUSTED METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31d55b",
   "metadata": {},
   "source": [
    "We merge opponent defensive strength into player records and calculate\n",
    "adjustment factors. This allows us to normalize performance metrics by\n",
    "the quality of opposition faced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'opponent_team' in df.columns:\n",
    "    # Create team ID to name mapping\n",
    "    team_id_map = df[['team', 'opponent_team']].drop_duplicates()\n",
    "    # This is tricky - opponent_team is numeric ID, need proper mapping\n",
    "    # For now, we'll skip opponent adjustment and note it needs team ID mapping\n",
    "    \n",
    "    print(\"Note: Opponent adjustment requires proper team ID to name mapping.\")\n",
    "    print(\"This will be implemented in the production feature engineering module.\")\n",
    "    print(\"The mathematical framework is established above.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9ad80d",
   "metadata": {},
   "source": [
    "## SECTION 6: FIXTURE DENSITY FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb681b5",
   "metadata": {},
   "source": [
    "Mathematical Foundation:\n",
    "\n",
    "Fixture density quantifies upcoming match congestion, which affects rotation\n",
    "probability and minutes expectation. For gameweek t, we count fixtures in\n",
    "forward-looking windows:\n",
    "\n",
    "    density_7d(t) = |{fixtures in [t, t+7 days]}|\n",
    "    density_14d(t) = |{fixtures in [t, t+14 days]}|\n",
    "\n",
    "High fixture density increases rotation risk, particularly for players at\n",
    "elite clubs with deep squads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb3a1f",
   "metadata": {},
   "source": [
    "## SECTION 7: POSITION-SPECIFIC FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4502b",
   "metadata": {},
   "source": [
    "Different positions exhibit distinct scoring patterns that require specialized\n",
    "feature engineering. We analyze position-specific distributions to inform\n",
    "feature construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze position-specific statistics\n",
    "print(\"Position-Specific Performance Distributions:\\n\")\n",
    "\n",
    "position_stats = df.groupby('position').agg({\n",
    "    'total_points': ['mean', 'std', 'median'],\n",
    "    'minutes': ['mean', 'median'],\n",
    "    'goals_scored': 'mean',\n",
    "    'assists': 'mean',\n",
    "    'clean_sheets': 'mean',\n",
    "    'ict_index': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(position_stats)\n",
    "\n",
    "# Visualize position-specific distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Position-Specific Performance Distributions', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Points distribution by position\n",
    "ax = axes[0, 0]\n",
    "df.boxplot(column='total_points', by='position', ax=ax)\n",
    "ax.set_title('Total Points by Position')\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Total Points')\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# ICT Index by position\n",
    "ax = axes[0, 1]\n",
    "df.boxplot(column='ict_index', by='position', ax=ax)\n",
    "ax.set_title('ICT Index by Position')\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('ICT Index')\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Minutes by position\n",
    "ax = axes[1, 0]\n",
    "df.boxplot(column='minutes', by='position', ax=ax)\n",
    "ax.set_title('Minutes Played by Position')\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Minutes')\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Goals + Assists by position\n",
    "ax = axes[1, 1]\n",
    "df['goal_contributions'] = df['goals_scored'] + df['assists']\n",
    "df.boxplot(column='goal_contributions', by='position', ax=ax)\n",
    "ax.set_title('Goal Contributions by Position')\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Goals + Assists')\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'notebooks' / 'figures' / 'position_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Midfielders show highest ICT index variation (offensive and defensive roles)\")\n",
    "print(\"- Forwards have highest scoring variance (feast or famine distribution)\")\n",
    "print(\"- Defenders show most consistent minutes (less rotation)\")\n",
    "print(\"- Goalkeepers have unique clean sheet dependency\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb2ef1",
   "metadata": {},
   "source": [
    "## SECTION 8: FEATURE CORRELATION ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ffb36",
   "metadata": {},
   "source": [
    "Examine correlations among engineered features to identify redundancy.\n",
    "High correlation indicates features capture overlapping information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa479a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rolling features for correlation analysis\n",
    "rolling_features = [c for c in df.columns if '_roll_' in c and '3gw' in c]\n",
    "core_features = ['total_points', 'ict_index', 'minutes', 'goals_scored', 'assists']\n",
    "\n",
    "analysis_cols = core_features + rolling_features[:8]  # Limit for readability\n",
    "analysis_cols = [c for c in analysis_cols if c in df.columns]\n",
    "\n",
    "# Calculate correlation matrix on non-null subset\n",
    "corr_df = df[analysis_cols].corr()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_df, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix - Core and Rolling Statistics', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR / 'notebooks' / 'figures' / 'feature_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelation insights:\")\n",
    "print(\"- Strong correlation between total_points and its rolling averages (expected)\")\n",
    "print(\"- ICT components show moderate correlation (capture different aspects)\")\n",
    "print(\"- Rolling windows of same metric highly correlated (expected temporal smoothing)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a051c",
   "metadata": {},
   "source": [
    "## SECTION 9: TARGET VARIABLE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d93fb27",
   "metadata": {},
   "source": [
    "Our prediction target is cumulative points over next 3-4 gameweeks.\n",
    "We construct this forward-looking target by summing future performance.\n",
    "\n",
    "Mathematical formulation:\n",
    "    Y(i,t) = Σ(k=1 to h) total_points(i, t+k)\n",
    "\n",
    "where h ∈ {3,4} is the forecast horizon.\n",
    "\n",
    "CRITICAL: This target cannot be used for training at gameweek t. It represents\n",
    "future information. We construct it here for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_forward_target(df: pd.DataFrame, horizon: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates forward-looking cumulative points target.\n",
    "    \n",
    "    This target represents the sum of points over the next `horizon` gameweeks.\n",
    "    It can only be used for validation on past data or for current prediction\n",
    "    where we are forecasting the unknown future.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Match data sorted temporally\n",
    "    horizon : int\n",
    "        Number of future gameweeks to sum\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with forward target column\n",
    "    \"\"\"\n",
    "    df_target = df.copy()\n",
    "    \n",
    "    # Calculate cumulative future points per player\n",
    "    df_target[f'target_points_{horizon}gw'] = (\n",
    "        df_target.groupby('element')['total_points']\n",
    "        .rolling(window=horizon, min_periods=1)\n",
    "        .sum()\n",
    "        .shift(-horizon + 1)  # Shift to align with current gameweek\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    \n",
    "    return df_target\n",
    "\n",
    "# Create targets for different horizons\n",
    "print(\"Creating forward-looking targets for horizons: 3, 4 gameweeks...\")\n",
    "\n",
    "df = create_forward_target(df, horizon=3)\n",
    "df = create_forward_target(df, horizon=4)\n",
    "\n",
    "# Validate target construction\n",
    "sample_player = df[df['element'] == active_players[0]].head(10)\n",
    "print(\"\\nSample Target Construction (first 10 gameweeks of sample player):\")\n",
    "print(sample_player[['GW', 'total_points', 'target_points_3gw', 'target_points_4gw']])\n",
    "\n",
    "print(\"\\nTarget variable statistics:\")\n",
    "for h in [3, 4]:\n",
    "    target_col = f'target_points_{h}gw'\n",
    "    print(f\"\\n{target_col}:\")\n",
    "    print(df[target_col].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e288dfc",
   "metadata": {},
   "source": [
    "## SECTION 10: SUMMARY AND NEXT STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eba53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFeatures Constructed:\")\n",
    "print(f\"1. Rolling performance metrics: {len([c for c in df.columns if '_roll_' in c])} features\")\n",
    "print(\"2. Position-specific analysis completed\")\n",
    "print(\"3. Target variables: target_points_3gw, target_points_4gw\")\n",
    "\n",
    "print(\"\\nFeatures Requiring Production Implementation:\")\n",
    "print(\"1. Opponent strength adjustment (needs team ID mapping)\")\n",
    "print(\"2. Fixture density metrics (needs fixture schedule API)\")\n",
    "print(\"3. Price-performance interaction features\")\n",
    "print(\"4. Lag features (previous GW performance)\")\n",
    "\n",
    "print(\"\\nCurrent Dataset Dimensions:\")\n",
    "print(f\"Rows: {len(df):,}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\nData Quality Check:\")\n",
    "rolling_cols = [c for c in df.columns if '_roll_' in c]\n",
    "for col in rolling_cols[:3]:\n",
    "    n_missing = df[col].isna().sum()\n",
    "    print(f\"{col:40s}: {n_missing:6d} missing ({n_missing/len(df)*100:5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING EXPLORATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c47437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save engineered features for next phase\n",
    "output_path = PROCESSED_DIR / \"fpl_features_exploratory.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\nEngineered features saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
