{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Feature Forensics & Audit\n",
    "\n",
    "**Objective:** Mathematically validate the engineered features from `02_feature_engineering.ipynb`.\n",
    "\n",
    "**Forensic Questions:**\n",
    "1. **Distribution Check:** Are the new features (EWMA, Interaction) distributed normally or do they have outliers?\n",
    "2. **Target Correlation:** Which features actually correlate with `target_points_next_3`?\n",
    "3. **Multicollinearity:** Did we create redundant features?\n",
    "4. **Visual Proof:** Does `upcoming_difficulty` actually align with points dropped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "BASE_DIR = Path.cwd().parent if 'Notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "PROCESSED_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "\n",
    "# Load the \"Model Ready\" dataset\n",
    "df = pd.read_csv(PROCESSED_DIR / \"fpl_features_production.csv\")\n",
    "print(f\"Evidence Loaded: {df.shape[0]:,} rows x {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corr_analysis",
   "metadata": {},
   "source": [
    "## 1. The Interrogation (Correlation Analysis)\n",
    "We check which features have the strongest linear relationship with the future target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corr_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for numeric columns only\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute correlations with the Target\n",
    "correlations = numeric_df.corrwith(df['target_points_next_3']).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 Positively Correlated Features:\")\n",
    "print(correlations.head(10))\n",
    "\n",
    "print(\"\\nTop 10 Negatively Correlated Features:\")\n",
    "print(correlations.tail(10))\n",
    "\n",
    "# Plotting the top 20 predictors\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = pd.concat([correlations.head(10), correlations.tail(10)])\n",
    "top_features.plot(kind='barh', color='teal')\n",
    "plt.title(\"Feature Importance (Linear Correlation with Target)\")\n",
    "plt.xlabel(\"Pearson Correlation Coefficient\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficulty_check",
   "metadata": {},
   "source": [
    "## 2. Evidence Validation: Does 'Difficulty' Matter?\n",
    "We visualize if our new `upcoming_difficulty_3gw` feature actually separates high performers from low performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficulty_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We bin the difficulty into categories for cleaner plotting\n",
    "df['difficulty_bin'] = pd.qcut(df['upcoming_difficulty_3gw'], q=5, labels=['Easiest', 'Easy', 'Medium', 'Hard', 'Hardest'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='difficulty_bin', y='target_points_next_3', showfliers=False)\n",
    "plt.title(\"Impact of Upcoming Fixture Difficulty on Future Points\")\n",
    "plt.xlabel(\"Average Opponent Defensive Strength (Next 3 Games)\")\n",
    "plt.ylabel(\"Actual Points Scored (Next 3 Games)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Forensic Note: If the boxplots trend DOWNWARDS as difficulty increases, the feature is valid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ewma_check",
   "metadata": {},
   "source": [
    "## 3. Signal vs. Noise: EWMA vs Rolling\n",
    "Does the Exponential Moving Average (EWMA) capture form better than the simple Rolling Mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ewma_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a volatile player (e.g., someone with high minutes variance)\n",
    "sample_player = df[df['minutes_cv_5'] > 0.5]['element'].iloc[0]\n",
    "player_data = df[df['element'] == sample_player].sort_values('GW')\n",
    "player_name = df[df['element'] == sample_player]['name'].iloc[0]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(player_data['GW'], player_data['total_points'], 'o', alpha=0.3, label='Raw Points', color='gray')\n",
    "plt.plot(player_data['GW'], player_data['total_points_roll_6'], '--', label='Rolling Mean (6)', color='blue')\n",
    "plt.plot(player_data['GW'], player_data['total_points_ewma_6'], '-', linewidth=2, label='EWMA (6)', color='red')\n",
    "\n",
    "plt.title(f\"Signal Processing: Rolling vs EWMA for {player_name}\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Points\")\n",
    "plt.xlabel(\"Gameweek\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stability_check",
   "metadata": {},
   "source": [
    "## 4. The \"Nailed\" Test (Stability Metrics)\n",
    "Validating if `minutes_cv_5` correctly identifies rotation risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stability_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df.sample(2000), x='minutes_cv_5', y='minutes_mean_5', alpha=0.5, hue='position')\n",
    "plt.title(\"Rotation Risk Analysis: Coefficient of Variation vs Mean Minutes\")\n",
    "plt.xlabel(\"Minutes Stability (CV) - Higher is Riskier\")\n",
    "plt.ylabel(\"Average Minutes (Last 5)\")\n",
    "plt.axvline(0.2, color='r', linestyle='--', label='Unstable Threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heatmap",
   "metadata": {},
   "source": [
    "## 5. Collinearity Audit\n",
    "Ensure we don't have redundant features that will confuse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heatmap_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = [\n",
    "    'total_points_ewma_6', 'total_points_roll_6', \n",
    "    'ict_index_ewma_6', 'value_efficiency', \n",
    "    'upcoming_difficulty_3gw', 'minutes_cv_5'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[cols_to_check].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
