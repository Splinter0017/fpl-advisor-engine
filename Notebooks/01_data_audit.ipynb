{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b66355",
   "metadata": {},
   "source": [
    "# Fantasy Premier League Data Audit Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8de391",
   "metadata": {},
   "source": [
    "This notebook performs comprehensive data quality assessment across all seasons,\n",
    "validating schema consistency, identifying missing values, and examining temporal\n",
    "coverage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186057d",
   "metadata": {},
   "source": [
    "## CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b2fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd().parent if 'Notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "RAW_DIR = BASE_DIR / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "\n",
    "HISTORICAL_SEASONS = [\"2021-22\", \"2022-23\", \"2023-24\", \"2024-25\"]\n",
    "CURRENT_SEASON = \"2025-26\"\n",
    "\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Raw Data Directory: {RAW_DIR}\")\n",
    "print(f\"Processed Data Directory: {PROCESSED_DIR}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb33e9",
   "metadata": {},
   "source": [
    "## SECTION 1: FILE INVENTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbdbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_files = {}\n",
    "\n",
    "for season in HISTORICAL_SEASONS + [CURRENT_SEASON]:\n",
    "    merged_file = RAW_DIR / f\"{season}_merged_gw.csv\"\n",
    "    teams_file = RAW_DIR / f\"{season}_teams.csv\"\n",
    "    \n",
    "    files_found = {\n",
    "        'merged_gw': merged_file.exists(),\n",
    "        'teams': teams_file.exists()\n",
    "    }\n",
    "    \n",
    "    available_files[season] = files_found\n",
    "    \n",
    "    status = \"COMPLETE\" if all(files_found.values()) else \"INCOMPLETE\"\n",
    "    print(f\"{season}: {status}\")\n",
    "    if not files_found['merged_gw']:\n",
    "        print(f\"  Missing: {season}_merged_gw.csv\")\n",
    "    if not files_found['teams']:\n",
    "        print(f\"  Missing: {season}_teams.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecc1b7",
   "metadata": {},
   "source": [
    "## SECTION 2: SCHEMA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3cf5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_info = {}\n",
    "\n",
    "for season in HISTORICAL_SEASONS + [CURRENT_SEASON]:\n",
    "    merged_file = RAW_DIR / f\"{season}_merged_gw.csv\"\n",
    "    \n",
    "    if not merged_file.exists():\n",
    "        print(f\"{season}: File not found, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(merged_file, nrows=5)\n",
    "        schema_info[season] = {\n",
    "            'columns': set(df.columns),\n",
    "            'n_columns': len(df.columns),\n",
    "            'dtypes': df.dtypes.to_dict()\n",
    "        }\n",
    "        print(f\"{season}: {len(df.columns)} columns detected\")\n",
    "    except Exception as e:\n",
    "        print(f\"{season}: Error reading file - {str(e)}\")\n",
    "        schema_info[season] = None\n",
    "# Identify common columns across all seasons\n",
    "if schema_info:\n",
    "    all_columns = [info['columns'] for info in schema_info.values() if info is not None]\n",
    "    \n",
    "    if all_columns:\n",
    "        common_columns = set.intersection(*all_columns)\n",
    "        all_unique_columns = set.union(*all_columns)\n",
    "        \n",
    "        print(f\"\\nCommon columns across all seasons: {len(common_columns)}\")\n",
    "        print(f\"Total unique columns across all seasons: {len(all_unique_columns)}\")\n",
    "        \n",
    "        # Identify columns that exist in some seasons but not others\n",
    "        inconsistent_columns = all_unique_columns - common_columns\n",
    "        if inconsistent_columns:\n",
    "            print(\"\\nInconsistent columns (not present in all seasons):\")\n",
    "            for col in sorted(inconsistent_columns):\n",
    "                seasons_with_col = [s for s, info in schema_info.items() \n",
    "                                   if info and col in info['columns']]\n",
    "                print(f\"  - {col}: present in {seasons_with_col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af153f38",
   "metadata": {},
   "source": [
    "## SECTION 3: DETAILED DATA PROFILING (First season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af12a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first available season for detailed inspection\n",
    "first_season = None\n",
    "for season in HISTORICAL_SEASONS:\n",
    "    merged_file = RAW_DIR / f\"{season}_merged_gw.csv\"\n",
    "    if merged_file.exists():\n",
    "        first_season = season\n",
    "        break\n",
    "\n",
    "if first_season:\n",
    "    print(f\"Analyzing {first_season} as reference season...\\n\")\n",
    "    \n",
    "    df_sample = pd.read_csv(RAW_DIR / f\"{first_season}_merged_gw.csv\")\n",
    "    \n",
    "    print(f\"Dataset Shape: {df_sample.shape[0]:,} rows × {df_sample.shape[1]} columns\")\n",
    "    print(f\"Memory Usage: {df_sample.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nColumn Inventory:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, col in enumerate(df_sample.columns, 1):\n",
    "        dtype = df_sample[col].dtype\n",
    "        n_missing = df_sample[col].isna().sum()\n",
    "        pct_missing = (n_missing / len(df_sample)) * 100\n",
    "        n_unique = df_sample[col].nunique()\n",
    "        \n",
    "        print(f\"{i:2d}. {col:25s} | {str(dtype):10s} | \"\n",
    "              f\"Missing: {n_missing:6d} ({pct_missing:5.2f}%) | \"\n",
    "              f\"Unique: {n_unique:6d}\")\n",
    "    \n",
    "    print(\"\\nTarget Variable Statistics (total_points):\")\n",
    "    print(\"-\" * 80)\n",
    "    if 'total_points' in df_sample.columns:\n",
    "        points_stats = df_sample['total_points'].describe()\n",
    "        print(points_stats)\n",
    "        \n",
    "        print(f\"\\nZero-point matches: {(df_sample['total_points'] == 0).sum():,} \"\n",
    "              f\"({(df_sample['total_points'] == 0).sum() / len(df_sample) * 100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nGameweek Coverage:\")\n",
    "    print(\"-\" * 80)\n",
    "    if 'GW' in df_sample.columns:\n",
    "        gw_coverage = df_sample['GW'].value_counts().sort_index()\n",
    "        print(f\"Gameweeks present: {gw_coverage.index.min()} to {gw_coverage.index.max()}\")\n",
    "        print(f\"Total gameweeks: {len(gw_coverage)}\")\n",
    "        print(\"\\nPlayers per gameweek (sample):\")\n",
    "        print(gw_coverage.head(10))\n",
    "    elif 'round' in df_sample.columns:\n",
    "        round_coverage = df_sample['round'].value_counts().sort_index()\n",
    "        print(f\"Rounds present: {round_coverage.index.min()} to {round_coverage.index.max()}\")\n",
    "        print(f\"Total rounds: {len(round_coverage)}\")\n",
    "    \n",
    "    print(\"\\nPosition Distribution:\")\n",
    "    print(\"-\" * 80)\n",
    "    if 'position' in df_sample.columns:\n",
    "        pos_dist = df_sample['position'].value_counts()\n",
    "        print(pos_dist)\n",
    "    elif 'element_type' in df_sample.columns:\n",
    "        element_dist = df_sample['element_type'].value_counts()\n",
    "        print(\"Element Type Distribution (1=GK, 2=DEF, 3=MID, 4=FWD):\")\n",
    "        print(element_dist)\n",
    "    \n",
    "    print(\"\\nKey Performance Metrics - Sample Statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    key_metrics = ['total_points', 'minutes', 'goals_scored', 'assists', \n",
    "                   'clean_sheets', 'ict_index', 'influence', 'creativity', 'threat']\n",
    "    \n",
    "    available_metrics = [m for m in key_metrics if m in df_sample.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        summary = df_sample[available_metrics].describe().T\n",
    "        summary['missing'] = df_sample[available_metrics].isna().sum()\n",
    "        summary['missing_pct'] = (summary['missing'] / len(df_sample)) * 100\n",
    "        print(summary[['count', 'mean', 'std', 'min', 'max', 'missing', 'missing_pct']])\n",
    "\n",
    "else:\n",
    "    print(\"No historical season data found for detailed profiling.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6e789",
   "metadata": {},
   "source": [
    "## SECTION 4: TEMPORAL COVERAGE SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_gameweeks = 0\n",
    "total_observations = 0\n",
    "\n",
    "for season in HISTORICAL_SEASONS + [CURRENT_SEASON]:\n",
    "    merged_file = RAW_DIR / f\"{season}_merged_gw.csv\"\n",
    "    \n",
    "    if not merged_file.exists():\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df_temp = pd.read_csv(merged_file)\n",
    "        \n",
    "        # Determine gameweek column\n",
    "        gw_col = 'GW' if 'GW' in df_temp.columns else 'round'\n",
    "        \n",
    "        if gw_col in df_temp.columns:\n",
    "            n_gw = df_temp[gw_col].nunique()\n",
    "            gw_range = f\"{df_temp[gw_col].min()}-{df_temp[gw_col].max()}\"\n",
    "        else:\n",
    "            n_gw = \"Unknown\"\n",
    "            gw_range = \"N/A\"\n",
    "        \n",
    "        n_obs = len(df_temp)\n",
    "        \n",
    "        print(f\"{season}: {n_obs:6,} observations across {n_gw} gameweeks (Range: {gw_range})\")\n",
    "        \n",
    "        if isinstance(n_gw, int):\n",
    "            total_gameweeks += n_gw\n",
    "        total_observations += n_obs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{season}: Error - {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal Temporal Coverage: {total_gameweeks} gameweeks\")\n",
    "print(f\"Total Observations: {total_observations:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72425e86",
   "metadata": {},
   "source": [
    "## SECTION 5: DATA QUALITY ISSUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_season:\n",
    "    df_quality = pd.read_csv(RAW_DIR / f\"{first_season}_merged_gw.csv\")\n",
    "    \n",
    "    print(\"Critical Fields Missing Value Analysis:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    critical_fields = ['total_points', 'minutes', 'element', 'opponent_team', 'was_home']\n",
    "    \n",
    "    for field in critical_fields:\n",
    "        if field in df_quality.columns:\n",
    "            n_missing = df_quality[field].isna().sum()\n",
    "            pct_missing = (n_missing / len(df_quality)) * 100\n",
    "            status = \"⚠ WARNING\" if pct_missing > 5 else \"✓ OK\"\n",
    "            print(f\"{field:20s}: {n_missing:6d} missing ({pct_missing:5.2f}%) {status}\")\n",
    "    \n",
    "    print(\"\\nDuplicate Records Check:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if 'element' in df_quality.columns and 'GW' in df_quality.columns:\n",
    "        n_duplicates = df_quality.duplicated(subset=['element', 'GW']).sum()\n",
    "        print(f\"Duplicate player-gameweek combinations: {n_duplicates}\")\n",
    "    elif 'element' in df_quality.columns and 'round' in df_quality.columns:\n",
    "        n_duplicates = df_quality.duplicated(subset=['element', 'round']).sum()\n",
    "        print(f\"Duplicate player-round combinations: {n_duplicates}\")\n",
    "    \n",
    "    print(\"\\nOutlier Detection (Total Points):\")\n",
    "    print(\"-\" * 80)\n",
    "    if 'total_points' in df_quality.columns:\n",
    "        q99 = df_quality['total_points'].quantile(0.99)\n",
    "        n_extreme = (df_quality['total_points'] > q99).sum()\n",
    "        print(f\"99th percentile: {q99:.1f} points\")\n",
    "        print(f\"Observations above 99th percentile: {n_extreme} ({n_extreme/len(df_quality)*100:.2f}%)\")\n",
    "        \n",
    "        if n_extreme > 0:\n",
    "            print(\"\\nTop 10 extreme performances:\")\n",
    "            extreme_cols = ['name', 'position', 'total_points', 'minutes', 'goals_scored', 'assists']\n",
    "            available_extreme = [c for c in extreme_cols if c in df_quality.columns]\n",
    "            if available_extreme:\n",
    "                print(df_quality.nlargest(10, 'total_points')[available_extreme].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"DATA AUDIT COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
