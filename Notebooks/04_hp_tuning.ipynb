{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d79bc602",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47f39a",
   "metadata": {},
   "source": [
    "Objective: Optimize LightGBM parameters for both classifier and regressor\n",
    "          components of the two-stage model.\n",
    "\n",
    "Strategy:\n",
    "- Bayesian optimization via Optuna\n",
    "- Tune classifier and regressor separately\n",
    "- Focus on Horizon 1 (transfer insights to H2/H3)\n",
    "- Target: 3-7% MAE improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a3f6d8",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score\n",
    "import optuna\n",
    "from optuna.visualization.matplotlib import plot_optimization_history, plot_param_importances\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Load preprocessed data from Notebook 04\n",
    "BASE_DIR = Path.cwd().parent\n",
    "PROCESSED_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "FEATURES_FILE = PROCESSED_DIR / \"fpl_features_engineered.csv\"\n",
    "\n",
    "df = pd.read_csv(FEATURES_FILE)\n",
    "print(f\"Loaded: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36603cc6",
   "metadata": {},
   "source": [
    "## 1. Prepare Data (from Notebook 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b798df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define feature groups based on your naming conventions\n",
    "engineered_features = [\n",
    "    col for col in df.columns \n",
    "    if any(x in col for x in [\n",
    "        '_roll_',          # Rolling means/stds from feature_engineering.py\n",
    "        '_ewm_',           # Exponential weighted moving averages\n",
    "        'form_',           # Form scores (z-scores)\n",
    "        'streak',          # Scoring/blank streaks\n",
    "        'fdr_',            # Fixture difficulty ratings\n",
    "        'home_away_',      # Home/Away delta\n",
    "        'position_adj',    # Position adjusted scores\n",
    "        'defensive_score', \n",
    "        'attacking_score'\n",
    "    ])\n",
    "]\n",
    "\n",
    "# 2. Add notebook-specific features (ROI) if not present\n",
    "# (Only calculate what isn't in feature_engineering.py)\n",
    "if 'roi_roll_5' not in df.columns:\n",
    "    df['roi_roll_5'] = (df['total_points_roll_mean_5'] / df['value']) * 10\n",
    "    df['roi_roll_3'] = (df['total_points_roll_mean_3'] / df['value']) * 10\n",
    "\n",
    "# 3. Add Contextual/Categorical Features\n",
    "context_features = [\n",
    "    'is_starter', \n",
    "    'was_home', \n",
    "    'position_encoded',\n",
    "    'value'\n",
    "]\n",
    "\n",
    "# Encode position if not already done\n",
    "if 'position_encoded' not in df.columns:\n",
    "    df['position_encoded'] = df['position'].map({'GK': 0, 'DEF': 1, 'MID': 2, 'FWD': 3})\n",
    "\n",
    "# 4. Final Feature List\n",
    "# Combine lists and remove duplicates/targets\n",
    "feature_cols = list(set(engineered_features + context_features + ['roi_roll_5', 'roi_roll_3']))\n",
    "\n",
    "# Filter out non-numeric or ID columns just in case\n",
    "exclude_cols = ['element', 'team', 'opponent_team', 'name', 'kickoff_time', 'season', 'GW']\n",
    "feature_cols = [c for c in feature_cols if c not in exclude_cols and c in df.columns]\n",
    "\n",
    "print(f\"Final Feature Count: {len(feature_cols)}\")\n",
    "for f in sorted(feature_cols)[:10]:\n",
    "    print(f\" - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal split\n",
    "def create_temporal_split(df):\n",
    "    train_mask = df['season'].isin(['2021-22', '2022-23', '2023-24'])\n",
    "    val_mask = (df['season'] == '2024-25') & (df['GW'] >= 15)\n",
    "    test_mask = (df['season'] == '2025-26') & (df['GW'] <= 12)\n",
    "    \n",
    "    return df[train_mask].copy(), df[val_mask].copy(), df[test_mask].copy()\n",
    "\n",
    "train_df, val_df, test_df = create_temporal_split(df)\n",
    "\n",
    "# Create targets\n",
    "def create_targets(df, horizons=[1, 2, 3]):\n",
    "    df = df.sort_values(['element', 'season', 'GW']).copy()\n",
    "    for h in horizons:\n",
    "        df[f'target_h{h}'] = df.groupby('element')['total_points'].shift(-h)\n",
    "        df[f'will_play_h{h}'] = (df.groupby('element')['minutes'].shift(-h) > 0).astype(int)\n",
    "    df = df.dropna(subset=[f'target_h{h}' for h in horizons])\n",
    "    return df\n",
    "\n",
    "train_df = create_targets(train_df)\n",
    "val_df = create_targets(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "def prepare_features(df, feature_cols):\n",
    "    X = df[feature_cols].copy()\n",
    "    if 'position' in X.columns:\n",
    "        X['position'] = X['position'].map({'GK': 0, 'DEF': 1, 'MID': 2, 'FWD': 3})\n",
    "    if 'was_home' in X.columns:\n",
    "        X['was_home'] = X['was_home'].fillna(0).astype(int)\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if X[col].isna().any():\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "    X = X.fillna(0)\n",
    "    return X.values\n",
    "\n",
    "X_train = prepare_features(train_df, feature_cols)\n",
    "X_val = prepare_features(val_df, feature_cols)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b91c55",
   "metadata": {},
   "source": [
    "## 2. Baseline Performance (From Notebook 03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef635b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline two-stage model for comparison\n",
    "horizon = 1\n",
    "\n",
    "y_train = train_df[f'target_h{horizon}'].values\n",
    "y_val = val_df[f'target_h{horizon}'].values\n",
    "y_train_binary = train_df[f'will_play_h{horizon}'].values\n",
    "y_val_binary = val_df[f'will_play_h{horizon}'].values\n",
    "\n",
    "# Baseline classifier\n",
    "clf_baseline = lgb.LGBMClassifier(n_estimators=300, learning_rate=0.05, random_state=42, verbose=-1)\n",
    "clf_baseline.fit(X_train, y_train_binary)\n",
    "prob_baseline = clf_baseline.predict_proba(X_val)[:, 1]\n",
    "auc_baseline = roc_auc_score(y_val_binary, prob_baseline)\n",
    "\n",
    "# Baseline regressor\n",
    "playing_mask = train_df['minutes'] > 0\n",
    "X_train_playing = X_train[playing_mask]\n",
    "y_train_playing = y_train[playing_mask]\n",
    "\n",
    "reg_baseline = lgb.LGBMRegressor(n_estimators=500, learning_rate=0.05, random_state=42, verbose=-1)\n",
    "reg_baseline.fit(X_train_playing, y_train_playing)\n",
    "points_baseline = np.maximum(reg_baseline.predict(X_val), 0)\n",
    "\n",
    "# Ensemble\n",
    "pred_baseline = prob_baseline * points_baseline\n",
    "mae_baseline = mean_absolute_error(y_val, pred_baseline)\n",
    "\n",
    "print(\"Baseline Two-Stage Model (Horizon 1):\")\n",
    "print(f\"  Classifier AUC: {auc_baseline:.4f}\")\n",
    "print(f\"  Ensemble MAE:   {mae_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee43cab",
   "metadata": {},
   "source": [
    "## 3. Optimize Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf1d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_classifier(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective for classifier optimization.\n",
    "    Maximizes AUC score.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train_binary)\n",
    "    \n",
    "    prob = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val_binary, prob)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"OPTIMIZING CLASSIFIER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "study_clf = optuna.create_study(direction='maximize', study_name='classifier_optimization')\n",
    "study_clf.optimize(objective_classifier, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest AUC: {study_clf.best_value:.4f}\")\n",
    "print(f\"Improvement: {(study_clf.best_value - auc_baseline) / auc_baseline * 100:.2f}%\")\n",
    "print(\"\\nBest parameters:\")\n",
    "for key, value in study_clf.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7343d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization\n",
    "plot_optimization_history(study_clf)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plot_param_importances(study_clf)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05170fbc",
   "metadata": {},
   "source": [
    "## 4. Optimize Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ce67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_regressor(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective for regressor optimization.\n",
    "    Minimizes MAE on validation set.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train_playing, y_train_playing)\n",
    "    \n",
    "    pred = np.maximum(model.predict(X_val), 0)\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d53aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"OPTIMIZING REGRESSOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "study_reg = optuna.create_study(direction='minimize', study_name='regressor_optimization')\n",
    "study_reg.optimize(objective_regressor, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest MAE: {study_reg.best_value:.4f}\")\n",
    "print(\"\\nBest parameters:\")\n",
    "for key, value in study_reg.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa83980",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study_reg)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plot_param_importances(study_reg)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640d042",
   "metadata": {},
   "source": [
    "## 5. Train Final Optimized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce87731",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAINING OPTIMIZED TWO-STAGE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Optimal classifier\n",
    "best_clf_params = study_clf.best_params\n",
    "best_clf_params.update({'objective': 'binary', 'metric': 'auc', 'verbosity': -1, 'random_state': 42})\n",
    "clf_optimized = lgb.LGBMClassifier(**best_clf_params)\n",
    "clf_optimized.fit(X_train, y_train_binary)\n",
    "\n",
    "prob_optimized = clf_optimized.predict_proba(X_val)[:, 1]\n",
    "auc_optimized = roc_auc_score(y_val_binary, prob_optimized)\n",
    "\n",
    "print(f\"Classifier AUC: {auc_optimized:.4f} (baseline: {auc_baseline:.4f})\")\n",
    "\n",
    "# Optimal regressor\n",
    "best_reg_params = study_reg.best_params\n",
    "best_reg_params.update({'objective': 'regression', 'metric': 'mae', 'verbosity': -1, 'random_state': 42})\n",
    "reg_optimized = lgb.LGBMRegressor(**best_reg_params)\n",
    "reg_optimized.fit(X_train_playing, y_train_playing)\n",
    "\n",
    "points_optimized = np.maximum(reg_optimized.predict(X_val), 0)\n",
    "\n",
    "# Ensemble\n",
    "pred_optimized = prob_optimized * points_optimized\n",
    "mae_optimized = mean_absolute_error(y_val, pred_optimized)\n",
    "\n",
    "print(f\"Ensemble MAE:   {mae_optimized:.4f} (baseline: {mae_baseline:.4f})\")\n",
    "print(f\"Improvement:    {(mae_baseline - mae_optimized) / mae_baseline * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c465f",
   "metadata": {},
   "source": [
    "## 6. Apply to All Horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"OPTIMIZED MODEL: ALL HORIZONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_optimized = {}\n",
    "\n",
    "for h in [1, 2, 3]:\n",
    "    y_train_h = train_df[f'target_h{h}'].values\n",
    "    y_val_h = val_df[f'target_h{h}'].values\n",
    "    y_train_binary_h = train_df[f'will_play_h{h}'].values\n",
    "    y_val_binary_h = val_df[f'will_play_h{h}'].values\n",
    "    \n",
    "    # Classifier\n",
    "    clf = lgb.LGBMClassifier(**best_clf_params)\n",
    "    clf.fit(X_train, y_train_binary_h)\n",
    "    prob = clf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Regressor\n",
    "    playing_mask_h = train_df['minutes'] > 0\n",
    "    X_train_playing_h = X_train[playing_mask_h]\n",
    "    y_train_playing_h = y_train_h[playing_mask_h]\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(**best_reg_params)\n",
    "    reg.fit(X_train_playing_h, y_train_playing_h)\n",
    "    points = np.maximum(reg.predict(X_val), 0)\n",
    "    \n",
    "    # Ensemble\n",
    "    pred = prob * points\n",
    "    mae = mean_absolute_error(y_val_h, pred)\n",
    "    \n",
    "    print(f\"\\nHorizon {h}: MAE = {mae:.4f}\")\n",
    "    \n",
    "    results_optimized[h] = {'mae': mae, 'predictions': pred}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33112351",
   "metadata": {},
   "source": [
    "## 7. Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results from Notebook 03\n",
    "baseline_maes = [1.021, 1.055, 1.073]  # Update with your actual values\n",
    "optimized_maes = [results_optimized[h]['mae'] for h in [1, 2, 3]]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Baseline': baseline_maes,\n",
    "    'Optimized': optimized_maes,\n",
    "    'Improvement_%': [(b - o) / b * 100 for b, o in zip(baseline_maes, optimized_maes)]\n",
    "}, index=['Horizon 1', 'Horizon 2', 'Horizon 3'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, baseline_maes, width, label='Baseline', color='coral', edgecolor='black', alpha=0.8)\n",
    "ax.bar(x + width/2, optimized_maes, width, label='Optimized', color='steelblue', edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Forecast Horizon', fontsize=12)\n",
    "ax.set_ylabel('Mean Absolute Error (MAE)', fontsize=12)\n",
    "ax.set_title('Hyperparameter Optimization Impact', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Horizon 1', 'Horizon 2', 'Horizon 3'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e34114",
   "metadata": {},
   "source": [
    "## 8. Save Optimized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2199a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save models for each horizon\n",
    "for h in [1, 2, 3]:\n",
    "    # Train final models with full pipeline\n",
    "    y_train_h = train_df[f'target_h{h}'].values\n",
    "    y_train_binary_h = train_df[f'will_play_h{h}'].values\n",
    "    \n",
    "    clf_final = lgb.LGBMClassifier(**best_clf_params)\n",
    "    clf_final.fit(X_train, y_train_binary_h)\n",
    "    \n",
    "    playing_mask = train_df['minutes'] > 0\n",
    "    reg_final = lgb.LGBMRegressor(**best_reg_params)\n",
    "    reg_final.fit(X_train[playing_mask], y_train_h[playing_mask])\n",
    "    \n",
    "    # Save\n",
    "    joblib.dump(clf_final, MODELS_DIR / f\"classifier_h{h}.pkl\")\n",
    "    joblib.dump(reg_final, MODELS_DIR / f\"regressor_h{h}.pkl\")\n",
    "\n",
    "# Save feature list\n",
    "joblib.dump(feature_cols, MODELS_DIR / \"feature_cols.pkl\")\n",
    "\n",
    "print(f\"\\nModels saved to: {MODELS_DIR}\")\n",
    "print(\"  - classifier_h1.pkl, classifier_h2.pkl, classifier_h3.pkl\")\n",
    "print(\"  - regressor_h1.pkl, regressor_h2.pkl, regressor_h3.pkl\")\n",
    "print(\"  - feature_cols.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
